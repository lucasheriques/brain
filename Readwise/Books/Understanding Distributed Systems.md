# Understanding Distributed Systems

![rw-book-cover](https://m.media-amazon.com/images/I/61otEO9SSYL._SY160.jpg)

## Metadata
- Author: [[Roberto Vitillo]]
- Full Title: Understanding Distributed Systems
- Category: #books

## Highlights
- distributed system is a group of nodes that cooperate by exchanging messages over communication links to achieve some task. A node can generically refer to a physical machine, like a phone, or a software process, like a browser. ([Location 47](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=47))
- Another reason for building distributed systems is that some applications require high availability and need to be resilient to single-node failures. ([Location 51](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=51))
- Some applications need to tackle workloads that are just too big to fit on a single node, no matter how powerful. ([Location 53](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=53))
- And finally, some applications have performance requirements that would be physically impossible to achieve with a single node. ([Location 54](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=54))
- Although it would be convenient to assume that some networking library is going to abstract all communication concerns away, in practice, it’s not that simple because abstractions leak1, and you need to understand how the network stack works when that happens. ([Location 61](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=61))
- Another hard challenge of building distributed systems is that some form of coordination is required to make individual nodes work in unison towards a shared objective. This is particularly challenging to do in the presence of failures. The “two generals” problem is a famous thought experiment that showcases this. ([Location 64](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=64))
- The performance of an application represents how efficiently it can handle load. Intuitively, load is anything that consumes the system’s resources such as CPU, memory, and network bandwidth. Since the nature of load depends on the application’s use cases and architecture, there are different ways to measure it. ([Location 75](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=75))
- Throughput is the number of requests processed per second by the application, while response time is the time elapsed in seconds between sending a request to the application and receiving a response. ([Location 79](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=79))
- The capacity of a distributed system depends on its architecture, its implementation, and an intricate web of physical limitations like the nodes’ memory size and clock cycle and the bandwidth and latency of network links. ([Location 86](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=86))
- For an application to be scalable, a load increase should not degrade the application’s performance. This requires increasing the capacity of the application at will. ([Location 88](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=88))
- A quick and easy way is to buy more expensive hardware with better performance, which is also referred to as scaling up. Unfortunately, this approach is bound to hit a brick wall sooner or later when such hardware just doesn’t exist. The alternative is scaling out by adding more commodity machines to the system and having them work together. ([Location 89](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=89))
- A distributed system is resilient when it can continue to do its job even when failures happen. And at scale, anything that can go wrong will go wrong. Every component has a probability of failing — nodes can crash, network links can be severed, etc. ([Location 96](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=96))
- Failures that are left unchecked can impact the system’s availability3, i.e., the percentage of time the system is available for use. ([Location 100](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=100))
- Availability is often described with nines, a shorthand way of expressing percentages of availability. Three nines are typically considered acceptable by users, and anything above four is considered to be highly available. ([Location 103](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=103))
- If the system isn’t resilient to failures, its availability will inevitably drop. ([Location 108](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=108))
- It’s a well-known fact that the majority of the cost of software is spent after its initial development in maintenance activities, such as fixing bugs, adding new features, and operating it. ([Location 111](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=111))
- Any change is a potential incident waiting to happen. Good testing — in the form of unit, integration, and end-to-end tests — is a minimum requirement to modify or extend a system without worrying it will break. And once a change has been merged into the codebase, it needs to be released to production safely without affecting the system’s availability. ([Location 113](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=113))
- Also, operators need to monitor the system’s health, investigate degradations and restore the service when it can’t self-heal. This requires altering the system’s behavior without code changes, e.g., toggling a feature flag or scaling out a service with a configuration change. ([Location 115](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=115))
- However, from a run-time point of view, a distributed system is a group of software processes that communicate via inter-process communication (IPC) mechanisms like HTTP. And from an implementation perspective, a distributed system is a group of loosely-coupled components (services) that communicate via APIs. ([Location 123](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=123))
- A service implements one specific part of the overall system’s capabilities. At the core of a service sits the business logic, which exposes interfaces to communicate with the outside world. ([Location 126](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=126))
- Since processes can’t call each other’s interfaces directly, adapters are needed to connect IPC mechanisms to service interfaces. ([Location 129](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=129))
- This architectural style is also referred to as the ports and adapters architecture4. The idea is that the business logic doesn’t depend on technical details; instead, the technical details depend on the business logic (dependency inversion principle5). ([Location 132](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=132))
- Communication between processes over the network, or inter-process communication (IPC), is at the heart of distributed systems — it’s what makes distributed systems distributed. In order for processes to communicate, they need to agree on a set of rules that determine how data is processed and formatted. Network protocols specify such rules. ([Location 155](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=155))
- The protocols are arranged in a stack1, where each layer builds on the abstraction provided by the layer below, and lower layers are closer to the hardware. ([Location 157](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=157))
- The link layer consists of network protocols that operate on local network links, like Ethernet or Wi-Fi, and provides an interface to the underlying network hardware. ([Location 161](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=161))
- The internet layer routes packets from one machine to another across the network. The Internet Protocol (IP) is the core protocol of this layer, which delivers packets on a best-effort basis (i.e., packets can be dropped, duplicated, or corrupted). ([Location 163](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=163))
- The transport layer transmits data between two processes. To enable multiple processes hosted on the same machine to communicate at the same time, port numbers are used to address the processes on either end. ([Location 166](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=166))
- Finally, the application layer defines high-level communication protocols, like HTTP or DNS. ([Location 168](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=168))
- Even though each protocol builds on top of another, sometimes the abstractions leak. ([Location 169](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=169))
- If you don’t have a good grasp of how the lower layers work, you will have a hard time troubleshooting networking issues that will inevitably arise. ([Location 170](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=170))
- At the internet layer, the communication between two nodes happens by routing packets to their destination from one router to the next. Two ingredients are required for this: a way to address nodes and a mechanism to route packets across routers. ([Location 186](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=186))
- Addressing is handled by the IP protocol. ([Location 188](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=188))
- Now, IP doesn’t guarantee that data sent over the internet will arrive at its destination. For example, if a router becomes overloaded, it might start dropping packets. This is where TCP2 comes in, a transport-layer protocol that exposes a reliable communication channel between two processes on top of IP. ([Location 192](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=192))
- A connection needs to be opened before any data can be transmitted on a TCP channel. The operating system manages the connection state on both ends through a socket. The socket keeps track of the state changes of the connection during its lifetime. ([Location 201](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=201))
- A server must be listening for connection requests from clients before a connection is established. TCP uses a three-way handshake to create a new connection, as shown in Figure 2.1: The sender picks a random sequence number x and sends a SYN segment to the receiver. The receiver increments x, chooses a random sequence number y, and sends back a SYN/ACK segment. The sender increments both sequence numbers and replies with an ACK segment and the first bytes of application data. ([Location 206](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=206))
- closing a socket doesn’t dispose of it immediately as it transitions to a waiting state (TIME_WAIT) that lasts several minutes and discards any segments received during the wait. The wait prevents delayed segments from a closed connection from being considered part of a new connection. But if many connections open and close quickly, the number of sockets in the waiting state will continue to increase until it reaches the maximum number of sockets that can be open, causing new connection attempts to fail. This is another reason why processes typically maintain connection pools to avoid recreating connections repeatedly. ([Location 216](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=216))
- Flow control is a backoff mechanism that TCP implements to prevent the sender from overwhelming the receiver. The receiver stores incoming TCP segments waiting to be processed by the application into a receive buffer, as shown in Figure 2.2. ([Location 221](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=221))
- The receiver also communicates the size of the buffer to the sender whenever it acknowledges a segment, as shown in Figure 2.3. Assuming it’s respecting the protocol, the sender avoids sending more data than can fit in the receiver’s buffer. ([Location 224](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=224))
- TCP guards not only against overwhelming the receiver, but also against flooding the underlying network. The sender maintains a so-called congestion window, which represents the total number of outstanding segments that can be sent without an acknowledgment from the other side. The smaller the congestion window is, the fewer bytes can be in flight at any given time, and the less bandwidth is utilized. ([Location 231](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=231))
- When a new connection is established, the size of the congestion window is set to a system default. Then, for every segment acknowledged, the window increases its size exponentially until it reaches an upper limit. This means we can’t use the network’s full capacity right after a connection is established. The shorter the round-trip time (RTT), the quicker the sender can start utilizing the underlying network’s bandwidth, as shown in Figure 2.4. ([Location 234](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=234))
- Because the sender needs to wait for a full round trip to get an acknowledgment, we can derive the maximum theoretical bandwidth by dividing the size of the congestion window by the round trip time: The equation5 shows that bandwidth is a function of latency. TCP will try very hard to optimize the window size since it can’t do anything about the round-trip time. ([Location 243](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=243))
- TCP’s reliability and stability come at the price of lower bandwidth and higher latencies than the underlying network can deliver. If we drop the stability and reliability mechanisms that TCP provides, what we get is a simple protocol named User Datagram Protocol6 (UDP) — a connectionless transport layer protocol that can be used as an alternative to TCP. ([Location 249](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=249))
- Unlike TCP, UDP does not expose the abstraction of a byte stream to its clients. As a result, clients can only send discrete packets with a limited size called datagrams. UDP doesn’t offer any reliability as datagrams don’t have sequence numbers and are not acknowledged. UDP doesn’t implement flow and congestion control either. Overall, UDP is a lean and bare-bones protocol. It’s used to bootstrap custom protocols, which provide some, but not all, of the stability and reliability guarantees that TCP does ([Location 252](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=252))
- TLS runs on top of TCP and encrypts the communication channel so that application layer protocols, like HTTP, can leverage it to communicate securely. In a nutshell, TLS provides encryption, authentication, and integrity. ([Location 276](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=276))
- When the TLS connection is first opened, the client and the server negotiate a shared encryption secret using asymmetric encryption. ([Location 280](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=280))
- Although asymmetric encryption is slow and expensive, it’s only used to create the shared encryption key. After that, symmetric encryption is used, which is fast and cheap. The shared key is periodically renegotiated to minimize the amount of data that can be deciphered if the shared key is broken. ([Location 283](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=283))
- TLS implements authentication using digital signatures based on asymmetric cryptography. The server generates a key pair with a private and a public key and shares its public key with the client. When the server sends a message to the client, it signs it with its private key. The client uses the server’s public key to verify that the digital signature was actually signed with the private key. This is possible thanks to mathematical properties3 of the key pair. ([Location 290](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=290))
- When a TLS connection is opened, the server sends the full certificate chain to the client, starting with the server’s certificate and ending with the root CA. The client verifies the server’s certificate by scanning the certificate chain until it finds a certificate that it trusts. Then, the certificates are verified in reverse order from that point in the chain. ([Location 302](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=302))
- TLS verifies the integrity of the data by calculating a message digest. A secure hash function is used to create a message authentication code5 (HMAC). When a process receives a message, it recomputes the digest of the message and checks whether it matches the digest included in the message. ([Location 311](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=311))
- The bottom line is that creating a new connection is not free: yet another reason to put your servers geographically closer to the clients and reuse connections when possible. ([Location 328](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=328))
- How do these caches know when to expire a record? Every DNS record has a time to live (TTL) that informs the cache how long the entry is valid for. But there is no guarantee that clients play nicely and enforce the TTL. So don’t be surprised when you change a DNS entry and find out that a small number of clients are still trying to connect to the old address long after the TTL has expired. ([Location 366](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=366))
- Setting a TTL requires making a tradeoff. If you use a long TTL, many clients won’t see a change for a long time. But if you set it too short, you increase the load on the name servers and the average response time of requests because clients will have to resolve the hostname more often. ([Location 369](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=369))
- If your name server becomes unavailable for any reason, then the smaller the record’s TTL is, the higher the number of clients impacted will be. DNS can easily become a single point of failure — if your DNS name server is down and clients can’t find the IP address of your application, they won’t be able to connect it. This can lead to massive outages4. ([Location 371](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=371))
- The principle that a system should continue to function even when a dependency is impaired is also referred to as “static stability”; we will talk more about it in the resiliency part of the book. ([Location 376](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=376))
- Direct communication requires that both processes are up and running for the communication to succeed. However, sometimes this guarantee is either not needed or very hard to achieve, in which case indirect communication is a better fit. ([Location 390](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=390))
- A textual format like JSON1 is self-describing and human-readable, at the expense of increased verbosity and parsing overhead. On the other hand, a binary format like Protocol Buffers2 is leaner and more performant than a textual one at the expense of human readability. ([Location 398](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=398))
- Synchronous communication is inefficient, as it blocks threads that could be used to do something else. ([Location 403](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=403))
- Commonly used IPC technologies for request-response interactions are HTTP and gRPC ([Location 405](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=405))
- HTTP is a stateless protocol, which means that everything needed by a server to process a request needs to be specified within the request itself, without context from previous requests. HTTP uses TCP for the reliability guarantees discussed in chapter 2. When it runs on top of TLS, it’s also referred to as HTTPS. ([Location 421](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=421))
- HTTP 1.1 keeps a connection to a server open by default to avoid needing to create a new one for the next transaction. However, a new request can’t be issued until the response to the previous one has been received (aka head-of-line blocking or HOL blocking); in other words, the transactions have to be serialized. For example, a browser that needs to fetch several images to render an HTML page has to download them one at a time, which can be very inefficient. ([Location 424](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=424))
- HTTP 28 was designed from the ground up to address the main limitations of HTTP 1.1. It uses a binary protocol rather than a textual one, allowing it to multiplex multiple concurrent request-response transactions (streams) on the same connection. ([Location 430](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=430))
- HTTP 39 is the latest iteration of the HTTP standard, which is based on UDP and implements its own transport protocol to address some of TCP’s shortcomings10. For example, with HTTP 2, a packet loss over the TCP connection blocks all streams (HOL), but with HTTP 3 a packet loss interrupts only one stream, not all of them. ([Location 433](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=433))
- Request methods can be categorized based on whether they are safe and whether they are idempotent. A safe method should not have any visible side effects and can safely be cached. An idempotent method can be executed multiple times, and the end result should be the same as if it was executed just a single time. Idempotency is a crucial aspect of APIs, and we will talk more about it later in section 5.7 ([Location 467](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=467))
- An API starts out as a well-designed interface15. Slowly but surely, it will have to change to adapt to new use cases. The last thing we want to do when evolving an API is to introduce a breaking change that requires all clients to be modified at once, some of which we might have no control over. ([Location 559](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=559))
- REST APIs should be versioned to support breaking changes, e.g., by prefixing a version number in the URLs (/v1/products/). However, as a general rule of thumb, APIs should evolve in a backward-compatible way unless there is a very good reason. Although backward-compatible APIs tend not to be particularly elegant, they are practical. ([Location 567](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=567))
- For the server to detect that a request is a duplicate, it needs to be decorated with an idempotency key — a unique identifier (e.g., a UUID). The identifier could be part of a header, like Idempotency-Key in Stripe’s API18. For the server to detect duplicates, it needs to remember all the request identifiers it has seen by storing them in a database. When a request comes in, the server checks the database to see if the request ID is already present. If it’s not there, it adds the request identifier to the database and executes the request. Request identifiers don’t have to be stored indefinitely, and they can be purged after some time. ([Location 582](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=582))
- If the request identifiers and the resources managed by the server are stored in the same database, we can guarantee atomicity with ACID transactions19. In other words, we can wrap the product creation and request identifier log within the same database transaction in the POST handler. However, if the handler needs to make external calls to other services to handle the request, the implementation becomes a lot more challenging20, since it requires some form of coordination. ([Location 590](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=590))
- To summarize, an idempotent API makes it a lot easier to implement clients that are robust to failures, since they can assume that requests can be retried on failure without worrying about all the possible edge cases. ([Location 604](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=604))
- It’s all too easy to ignore the “leaking” complexity that goes into it, since modern programming languages and frameworks make it look like invoking a remote API is no different from calling a local function. I know I did at first and eventually learned this lesson after spending days investigating weird degradations that ultimately turned out to be caused by exhausted socket pools or routers dropping packets. ([Location 646](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=646))
- everything that can go wrong when a remote request is made: socket pools can be exhausted, TLS certificates can expire, DNS servers can become unavailable, routers can become congested and drop packets, non-idempotent requests can result in unexpected states, and the list goes on. The only universal truth is that the fastest, safest, most secure, and reliable network call is the one you don’t have to make. ([Location 649](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=649))
- Our ultimate goal is to build a distributed application made of a group of processes that gives its users the illusion they are interacting with one coherent node. Although achieving a perfect illusion is not always possible or desirable, some degree of coordination is always needed to build a distributed application. ([Location 656](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=656))
- To reason about distributed systems, we need to define precisely what can and can’t happen. A system model encodes expectations about the behavior of processes, communication links, and timing; think of it as a set of assumptions that allow us to reason about distributed systems by ignoring the complexity of the actual technologies used to implement them. ([Location 679](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=679))
- The arbitrary-fault model is typically used to model safety-critical systems like airplane engines, nuclear power plants, and systems where a single entity doesn’t fully control all the processes (e.g., digital cryptocurrencies such as Bitcoin). These use cases are outside the book’s scope, and the algorithms presented here will generally assume a crash-recovery model. ([Location 696](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=696))
- Several things can go wrong when a client sends a request to a server. In the best case, the client sends a request and receives a response. But what if no response comes back after some time? In that case, it’s impossible to tell whether the server is just very slow, it crashed, or a message couldn’t be delivered because of a network issue ([Location 721](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=721))
- The tricky part is deciding how long to wait for the timeout to trigger. If the delay is too short, the client might wrongly assume the server is unavailable; if the delay is too long, the client might waste time waiting for a response that will never arrive. In summary, it’s not possible to build a perfect failure detector. ([Location 729](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=729))
- Pings and heartbeats are generally used for processes that interact with each other frequently, in situations where an action needs to be taken as soon as one of them is no longer reachable. In other circumstances, detecting failures just at communication time is good enough. ([Location 737](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=737))
## New highlights added August 12, 2023 at 4:37 PM
- The flow of execution of a single-threaded application is simple to understand because every operation executes sequentially in time, one after the other. But in a distributed system, there is no shared global clock that all processes agree on that can be used to order operations. And, to make matters worse, processes can run concurrently. ([Location 742](https://readwise.io/to_kindle?action=open&asin=B09YLRB7QV&location=742))
